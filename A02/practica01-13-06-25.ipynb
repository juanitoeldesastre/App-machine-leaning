{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CLASIFICADOR CON AUMENTO DE DATOS (DATA AUGMENTATION) UTILIZANDO UNA RED CONVOLUCIONAL EN EL DATASET FASHION MNIST\n---\n## Objetivo del ejercicio\nAprender cómo mejorar la generalización de un modelo convolucional aplicando técnicas de\naumento de datos mediante ImageDataGenerator.\n## ¿Qué es ImageDataGenerator?\nImageDataGenerator es una clase de Keras (en TensorFlow) que se usa para preprocesar\nimágenes antes de entrenar una red neuronal. Su principal utilidad es permitir el aumento de\ndatos (data augmentation), lo que ayuda a que tu modelo aprenda mejor y generalice más.","metadata":{}},{"cell_type":"code","source":"# Paso 1: Cargar y preparar los datos\nfrom tensorflow.keras.datasets import fashion_mnist\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T00:42:25.156892Z","iopub.execute_input":"2025-06-14T00:42:25.157454Z","iopub.status.idle":"2025-06-14T00:42:25.625965Z","shell.execute_reply.started":"2025-06-14T00:42:25.157421Z","shell.execute_reply":"2025-06-14T00:42:25.624964Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"**¿Qué es Fashion MNIST?**\n\nEs un dataset de imágenes en escala de grises de 28x28 píxeles de ropa: camisetas, pantalones,\nzapatos, etc. Tiene 10 clases y es similar a MNIST, pero más complejo.","metadata":{}},{"cell_type":"code","source":"# Paso 2: Normalizar y agregar canal\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\nx_train = x_train.reshape(-1, 28, 28, 1)\nx_test = x_test.reshape(-1, 28, 28, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T00:42:25.627608Z","iopub.execute_input":"2025-06-14T00:42:25.627955Z","iopub.status.idle":"2025-06-14T00:42:25.759494Z","shell.execute_reply.started":"2025-06-14T00:42:25.627926Z","shell.execute_reply":"2025-06-14T00:42:25.758606Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**¿Por qué hacemos esto?**\n\n* Normalizamos los valores de píxeles para que estén entre 0 y 1 (esto ayuda alentrenamiento).\n* Añadimos un canal (1) para que TensorFlow entienda que son imágenes de un solocanal (grises).\n","metadata":{}},{"cell_type":"code","source":"# Paso 3: Codificar etiquetas con to_categorical\nfrom tensorflow.keras.utils import to_categorical\ny_train_cat = to_categorical(y_train, 10)\ny_test_cat = to_categorical(y_test, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T00:42:25.760322Z","iopub.execute_input":"2025-06-14T00:42:25.760667Z","iopub.status.idle":"2025-06-14T00:42:25.767501Z","shell.execute_reply.started":"2025-06-14T00:42:25.760643Z","shell.execute_reply":"2025-06-14T00:42:25.766604Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**¿Qué hace esto?**\n\nConvierte las etiquetas (por ejemplo 3) a un vector como [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. Es\nnecesario para clasificación multicategoría con softmax.","metadata":{}},{"cell_type":"code","source":"# Paso 4: Crear el generador con aumento de datos\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n rotation_range=15,\n width_shift_range=0.1,\n height_shift_range=0.1,\n zoom_range=0.1\n)\ndatagen.fit(x_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T00:42:25.768486Z","iopub.execute_input":"2025-06-14T00:42:25.768716Z","iopub.status.idle":"2025-06-14T00:42:25.848940Z","shell.execute_reply.started":"2025-06-14T00:42:25.768699Z","shell.execute_reply":"2025-06-14T00:42:25.848034Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**¿Qué hace ImageDataGenerator?**\n\nCrea versiones aumentadas (ligeramente modificadas) de las imágenes para simular\nvariabilidad real. Esto reduce el sobreajuste.\n\n* rotation_range=15: Rota la imagen aleatoriamente hasta ±15 grados.\n* width_shift_range=0.1: Desplaza horizontalmente hasta el 10% del ancho.\n* height_shift_range=0.1: Igual pero vertical.\n* zoom_range=0.1: Hace zoom aleatorio hasta un 10%.","metadata":{}},{"cell_type":"code","source":"# Paso 5: Visualizar imágenes aumentadas\nfor x_batch, y_batch in datagen.flow(x_train, y_train_cat, batch_size=9):\n # Muestra 9 imágenes aumentadas\n ...\n break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T00:42:25.850651Z","iopub.execute_input":"2025-06-14T00:42:25.850938Z","iopub.status.idle":"2025-06-14T00:42:25.873050Z","shell.execute_reply.started":"2025-06-14T00:42:25.850918Z","shell.execute_reply":"2025-06-14T00:42:25.871976Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"**¿Por qué lo visualizamos?**\n\nPara comprobar que el generador está funcionando y las imágenes se ven razonables (no\ndeformadas o mal etiquetadas).","metadata":{}},{"cell_type":"code","source":"# Paso 6: Crear modelo CNN simple\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nmodel = Sequential([\n Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n MaxPooling2D(pool_size=(2, 2)),\n Flatten(),\n Dense(64, activation='relu'),\n Dense(10, activation='softmax')\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T00:42:25.873968Z","iopub.execute_input":"2025-06-14T00:42:25.874232Z","iopub.status.idle":"2025-06-14T00:42:25.988922Z","shell.execute_reply.started":"2025-06-14T00:42:25.874206Z","shell.execute_reply":"2025-06-14T00:42:25.987936Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n2025-06-14 00:42:25.894988: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"**¿Qué hace esta red?**\n\nEs una red convolucional básica pero funcional:\n1. Capa convolucional (extrae características)\n2. Max Pooling (reduce dimensiones)\n3. Flatten (aplasta los datos para Dense)\n4. Capa densa oculta\n5. Capa de salida softmax (10 clases)","metadata":{}},{"cell_type":"code","source":"# Paso 7: Compilar y entrenar el modelo usando el generador\nmodel.compile(optimizer='adam',\n loss='categorical_crossentropy',\n metrics=['accuracy'])\nmodel.fit(datagen.flow(x_train, y_train_cat, batch_size=32),\n epochs=5,\n validation_data=(x_test, y_test_cat))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T00:42:25.989881Z","iopub.execute_input":"2025-06-14T00:42:25.990114Z","iopub.status.idle":"2025-06-14T00:45:06.604175Z","shell.execute_reply.started":"2025-06-14T00:42:25.990096Z","shell.execute_reply":"2025-06-14T00:45:06.603168Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 17ms/step - accuracy: 0.7002 - loss: 0.8138 - val_accuracy: 0.8276 - val_loss: 0.4641\nEpoch 2/5\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 17ms/step - accuracy: 0.8060 - loss: 0.5137 - val_accuracy: 0.8479 - val_loss: 0.4186\nEpoch 3/5\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 17ms/step - accuracy: 0.8191 - loss: 0.4751 - val_accuracy: 0.8576 - val_loss: 0.4016\nEpoch 4/5\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 17ms/step - accuracy: 0.8340 - loss: 0.4425 - val_accuracy: 0.8691 - val_loss: 0.3728\nEpoch 5/5\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 17ms/step - accuracy: 0.8406 - loss: 0.4186 - val_accuracy: 0.8720 - val_loss: 0.3566\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x784552fdc890>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"**¿Qué hace fit(datagen.flow(...))?**\n\nEntrena el modelo alimentándolo con las imágenes aumentadas generadas en tiempo real.","metadata":{}},{"cell_type":"code","source":"# Paso 8: Evaluar en los datos de prueba\ntest_loss, test_acc = model.evaluate(x_test, y_test_cat)\nprint(f\"Precisión en test: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T00:45:06.605155Z","iopub.execute_input":"2025-06-14T00:45:06.605453Z","iopub.status.idle":"2025-06-14T00:45:08.027713Z","shell.execute_reply.started":"2025-06-14T00:45:06.605422Z","shell.execute_reply":"2025-06-14T00:45:08.026836Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8735 - loss: 0.3602\nPrecisión en test: 0.8720\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"**¿Qué obtenemos?**\n\nUna métrica de rendimiento (precisión final) usando **datos reales de prueba no aumentados.** \nCompara este resultado con el mismo modelo sin aumento.","metadata":{}}]}